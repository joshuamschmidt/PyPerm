#!/usr/bin/env python
import argparse
import os
from pathlib import Path
import sys
import textwrap
from set_perm import set_perm as sp
import pandas as pd
import numpy as np
from statsmodels.stats.multitest import fdrcorrection



def tuple_type(strings):
    strings = strings.replace("(", "").replace(")", "")
    mapped_str = map(str, strings.split(","))
    return tuple(mapped_str)


parser = argparse.ArgumentParser(prog='set_perm',
                        formatter_class=argparse.RawDescriptionHelpFormatter,
                        description=textwrap.dedent('''\
                        Simple Gene Set enrichment test
                        --------------------------------------------
                        Generates a gene set enrichment test report file
                        for each input file/background sets.
                        
                        NOTES:
                        Number of background files must much number of candidate files.
                        Candidate SNP/Fatures must be a subset of background files.

                        !!!!!Python needs to be >=3.9!!!!!!
                        '''),
                        add_help=False,
                        epilog="Questions, bugs etc?\njoshmschmidt1@gmail.com\ngithub.com/joshuamschmidt")
parser._action_groups.pop()
required = parser.add_argument_group('required arguments')
optional = parser.add_argument_group('optional arguments')

# Add back help 
optional.add_argument(
    '-h',
    '--help',
    action='help',
    default=argparse.SUPPRESS,
    help='show this help message and exit'
)
'''required and optional argument parser'''

required.add_argument('--candidates', type=tuple_type, dest='c_files', nargs='+',
                    help='one or more candidate snp/feature files. Must be passed as a comma sep name,file tuple')

required.add_argument('--background', type=str, dest='b_file', nargs=1,
                    help='background snp/feature file')

required.add_argument('--feature_def', type=str, dest='feature_def',
                    help='file that defines features e.g. genes: chr start stop name')

required.add_argument('--function_def', type=str, dest='function_def',
                    help='file that defines function sets')

optional.add_argument('--min_set_size', dest='min_set_size', type=int,
                       help='set minium number of genes required in feature set. Default: 10')
parser.set_defaults(min_set_size=10)

optional.add_argument('--n_perms', dest='n_perms', type=int,
                       help='Number of random permuations for p-value estimation. Default: 10,000')
parser.set_defaults(n_perms=10000)

optional.add_argument('--prefix', dest='user_prefix', type=str,
                       help='Prefix identifyer for output file(s). Default: ""')
parser.set_defaults(user_prefix="")

parser.set_defaults(user_prefix=False)

optional.add_argument('--gene_def', dest='gene_def', type=int, default= 0,
                       help='an int to specifiy if feature defs are changed in bp. Currently modfies both start and end coordinates. Default=0')

optional.add_argument('--threads', type=int, dest='threads',
                       help='multithreaded mode with int threads')


def main():
    args = parser.parse_args()
    annotations = sp.AnnotationSet(annotation_file=args.feature_def, range_modification=args.gene_def)
    function_sets = sp.FunctionSets(function_set_file=args.function_def, min_set_size=args.min_set_size, annotation_obj=annotations)
    function_name =  Path(args.function_def).stem

    # begin
    c_files=args.c_files
    bg=args.b_file[0]
    nested_cand_objs = [None] * len(c_files)
    nested_names = [None] * len(c_files)
    n_genes_per_list = [None] * len(c_files)
    n_cand_lists = len(c_files)
    n_perms=args.n_perms
    threads=args.threads
    user_prefix=args.user_prefix
    bg_variants =  sp.Variants(variant_file=bg)
    bg_variants.annotate_variants(annotation_obj=annotations)


    for j, c_file in enumerate(c_files):
        name, cands = c_file
        nested_names[j] = name
        cand_variants = sp.Variants(variant_file=cands)
        cand_variants.annotate_variants(annotation_obj=annotations)
        nested_cand_objs[j]=cand_variants
        n_genes_per_list[j]=len(sp.get_idx_array(cand_variants.annotated_variants)[0])

    # now need to check that a: which cand object is biggest,
    # and b, that the other ones are a subset of the biggest.
    n_cand_variants_list = [obj.num_variants for obj in nested_cand_objs]
    index_by_list_size=[i[0] for i in sorted(enumerate(n_cand_variants_list), key=lambda k: k[1], reverse=True)]
    # c
    biggest=index_by_list_size[0]
    for i in range(1, len(index_by_list_size), 1):
        this_idx=index_by_list_size[i]
        if not nested_cand_objs[this_idx].is_subset_of(nested_cand_objs[biggest]):
            raise ValueError("candidates must be nested!")

    # should proably have a check that each nested list is unique size. and that
    # all smaller lists are subsets of all larger ones?
    # make test and perm objects from the biggest candidate gene set
    test_obj = sp.TestObject(nested_cand_objs[biggest], bg_variants, function_sets, annotations, n_cores = threads)
    perm_obj = sp.Permutation(test_obj, n_perms, threads)
    per_set = sp.SetPerPerm(perm_obj, function_sets, test_obj, threads)
    results = sp.make_nested_results_table(test_obj, function_sets, per_set, annotations)
    # now have to split perms into smaller subsets
    results_list= [None] * len(c_files)
    results_list[0]=results
    per_set_list= [None] * len(c_files)
    per_set_list[0]=per_set
    for i in range(1, len(index_by_list_size), 1):
        this_idx=index_by_list_size[i]
        this_perm=sp.Permutation.nested_perm(perm_obj, n_genes_per_list[this_idx])
        this_test = sp.TestObject.nested_test(nested_cand_objs[this_idx], function_sets, annotations)
        this_per_set = sp.SetPerPerm(this_perm, function_sets, this_test, threads)
        this_result=sp.make_nested_results_table(this_test, function_sets, this_per_set, annotations)
        results_list[i]=this_result
        per_set_list[i]=this_per_set

    merged_results=sp.combine_nested_results_table(results_list,per_set_list,nested_names, index_by_list_size)
    out_name="_".join(sorted(nested_names))
    sp.results_writer(merged_results, out_name, function_name, user_prefix)

   # t_fdr=fdrcorrection(merged_results.iloc[:,7].values, alpha=0.05, method='indep', is_sorted=False)

if __name__ == '__main__':
    main()
